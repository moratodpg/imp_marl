# --- FACMAC_smac specific parameters ---
#action_range: ~
action_selector: "gumbel"
epsilon_start: 0.3
epsilon_finish: 0.005
epsilon_anneal_time: 50000

runner: "episode"
batch_size: 64
buffer_size: 2000

agent: rnn # qmixrnn

agent_output_type: "pi_logits"

learner: "facmac_learner_discrete"
mac: facmac_mac
optimizer: "adam" # D
mixer: "qmix_facmac"
mixing_embed_dim: 64
skip_connections: False
gated: False
hypernet_layers: 2
hypernet_embed: 128
hyper_initialization_nonzeros: 0
td_lambda: 0.8
target_update_interval: 200
target_update_mode: 'hard'
target_update_tau: 0.001
q_embed_dim: 1
mask_before_softmax: True

name: "facmac"

use_cuda: False
use_tensorboard: True


#obs_agent_id: True # Include the agent's one_hot id in the observation
#obs_last_action: True # Include the agent's last action (one_hot) in the observation
#buffer_warmup: 0
#discretize_actions: False
#double_q: False
#epsilon_decay_mode: ~
#exploration_mode: "gaussian"
#start_steps: 0 # Number of steps for uniform-random action selection, before running real policy. Helps exploration.
#act_noise: 0.1 # Stddev for Gaussian exploration noise added to policy at training time.
#ou_theta: 0.15 # D
#ou_sigma: 0.2 # D
#ou_noise_scale: 0.3
#final_ou_noise_scale: 0.

#critic_train_reps: 1
#q_nstep: 0  # 0 corresponds to default Q, 1 is r + gamma*Q, etc

#n_runners: ~
#n_train: 1

#ou_stop_episode: 100 # training noise goes to zero after this episode

#run_mode: ~
#runner_scope: 'episodic'

#testing_on: True
#verbose: False
#weight_decay: True
#weight_decay_factor: 0.0001
#env_args:
#  state_last_action: False # critic adds last action internally
#agent_return_logits: False
